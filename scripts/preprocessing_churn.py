# -*- coding: utf-8 -*-
"""Preprocessing_churn.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ThxlDbv7DM74V6rxiCswOjzKtCDlbm10

# Preprocessamento

O Preprocessamento dos dados ocorre após a etapa de Análise Exploratória, na qual algumas transformações foram aplicadas à matriz de dados. Assim, a matriz carregada neste momento não corresponde aos dados brutos, mas sim à versão modificada após o EDA.

# Importação dos dados
"""

from google.colab import files

# Abre a janela para selecionar o arquivo
uploaded = files.upload()

# Depois, para ler o arquivo
import pandas as pd

matriz = pd.read_csv('matriz_EDA.csv')

"""# 1. Multicolinearidade - análise antes do escalonamento

## 1.1. Correlação de Pearson/Spearman - valores

Correlação de Pearson/Spearman - valores: a correlação de Pearson é usada para medir a relação linear entre duas variáveis numéricas, enquanto a Spearman mede a relação monotônica. Ambas fornecem valores numéricos que indicam a força e a direção da relação entre variáveis, como entre MonthlyCharges e TotalCharges.
"""

# Correlação de Pearson/Spearman - valores
# Pearson: avalia relação linear (boa para variáveis contínuas normalmente distribuídas).
# Spearman: avalia relação monótona (serve também para não lineares ou ordinais).
# Correlação alta entre variáveis independentes pode causar multicolinearidade em modelos preditivos (e aí entra o VIF).

# Correlação	Valor	Interpretação
# Positiva forte	+0.7 a +1.0	Ambas crescem juntas
# Positiva moderada	+0.4 a +0.69	Tendência de crescer juntas
# Fraca / quase nula	-0.3 a +0.3	Pouca ou nenhuma relação linear
# Negativa moderada	-0.4 a -0.69	Uma sobe enquanto a outra desce
# Negativa forte	-0.7 a -1.0	Relação inversa forte


# Seleciona apenas as colunas numéricas
numericas = matriz.select_dtypes(include='number')

# Correlação de Pearson
print("\nCorrelação de Pearson:")
print(numericas.corr(method='pearson'))

# Correlação de Spearman
print("\nCorrelação de Spearman:")
print(numericas.corr(method='spearman'))

"""## 1.2. Correlação de Pearson/Spearman - heatmap

Correlação de Pearson/Spearman - heatmap: exibe visualmente as correlações entre múltiplas variáveis, usando uma matriz de correlação com cores para indicar a intensidade da relação. Essa análise é útil para identificar rapidamente quais variáveis têm correlação forte ou fraca entre si, facilitando a compreensão das interações entre os dados.
"""

# Correlação de Pearson/Spearman -heatmap
import seaborn as sns
import matplotlib.pyplot as plt

# Seleciona colunas numéricas
numericas = matriz.select_dtypes(include='number')

# Heatmap da correlação de Pearson
plt.figure(figsize=(20, 14))
sns.heatmap(numericas.corr(method='pearson'), annot=True, cmap='coolwarm', fmt=".2f")
plt.title("Mapa de Correlação - Pearson")
plt.show()

# Heatmap da correlação de Spearman
plt.figure(figsize=(20, 14))
sns.heatmap(numericas.corr(method='spearman'), annot=True, cmap='YlGnBu', fmt=".2f")
plt.title("Mapa de Correlação - Spearman")
plt.show()

"""## 1.3. Análise de variáveis altamente correlacionadas

O código realiza uma análise de variáveis altamente correlacionadas no conjunto de dados. A ideia é identificar duplas de variáveis que possuem uma forte correlação, o que pode ser problemático em modelos de aprendizado de máquina ou regressão, pois pode levar a multicolinearidade e dificultar a interpretação dos resultados.
"""

#Análise de variáveis altamente correlacionadas
import numpy as np

# Threshold de correlação alta
threshold = 0.85

# Seleciona apenas colunas numéricas
numericas = matriz.select_dtypes(include='number')

# Matriz de correlação absoluta
cor_matrix = numericas.corr().abs()

# Pega a parte superior da matriz para evitar duplicações
upper_tri = cor_matrix.where(np.triu(np.ones(cor_matrix.shape), k=1).astype(bool))

# Identifica colunas com correlação acima do limiar
altamente_correlacionadas = [col for col in upper_tri.columns if any(upper_tri[col] > threshold)]

print(f"\nVariáveis altamente correlacionadas (r > {threshold}):")
print(altamente_correlacionadas)

"""## 1.4. Detecção de relações não-lineares

O pairplot ajuda a visualizar relações não-lineares entre as variáveis numéricas, o que é importante porque a correlação de Pearson (usada nas análises lineares) não captura relações não-lineares. Ou seja, ele permite perceber padrões não-lineares ou interações complexas entre as variáveis que podem ser importantes para a modelagem.

* Identificação de padrões não-lineares: Variáveis que não possuem uma correlação linear forte, mas que podem estar relacionadas de outras formas, como curvas ou outras formas não-lineares.

* Insights sobre os dados: Pode ajudar a identificar transformações de variáveis ou interações entre elas que podem ser úteis em modelos de aprendizado de máquina, como a criação de novas características ou o uso de técnicas específicas para capturar relações não-lineares (por exemplo, árvores de decisão).
"""

# Detecção de Relações Não-Lineares (via Pairplot + Spearman)
# Pairplot ajuda a visualizar relações não-lineares
sns.pairplot(matriz.select_dtypes(include='number'), kind='scatter', corner=True)
plt.suptitle("Pairplot para relações não-lineares", y=1.02)
plt.show()

"""## 1.5. Boxplots por classe do target

O código realiza a análise de variáveis numéricas em relação à variável alvo, 'Churn' (que provavelmente representa se o cliente abandonou ou não o serviço), utilizando boxplots para verificar a distribuição de cada variável numérica para cada classe do target (Churn).
"""

import seaborn as sns
import matplotlib.pyplot as plt

# Verifica se a variável 'Churn' está no DataFrame
if 'Churn' in matriz.columns:
    # Selecionar variáveis numéricas (excluindo o target se for numérico)
    variaveis_numericas = matriz.select_dtypes(include='number').columns.drop('Churn', errors='ignore')

    for var in variaveis_numericas:
        plt.figure(figsize=(10, 6))
        sns.boxplot(x='Churn', y=var, data=matriz)
        plt.title(f"Boxplot de {var} por classe do target (Churn)")
        plt.tight_layout()
        plt.show()
else:
    print("A variável 'Churn' não foi encontrada no DataFrame.")

"""## 1.6. Correlação com o target

O código calcula a correlação de Pearson entre a variável target ('Churn') e as variáveis numéricas do DataFrame, para identificar o grau de relacionamento linear entre cada variável numérica e o target.

A correlação de Pearson mede o grau de relacionamento linear entre duas variáveis. Ela varia entre -1 e 1, onde:

* 1 indica uma correlação positiva perfeita (à medida que uma variável aumenta, a outra também aumenta).
* -1 indica uma correlação negativa perfeita (à medida que uma variável aumenta, a outra diminui).
* 0 indica nenhuma correlação linear.
"""

# Correlação com o target (somente variáveis numéricas)
# Correlação de Pearson entre variáveis numéricas e o target
correlacoes = matriz.corr(numeric_only=True)['Churn'].drop('Churn').sort_values(ascending=False)
print("Correlação com o target:")
print(correlacoes)

"""# 2. Separação do dataset - treinamento e teste"""

# Separação dos dados em treinamento e teste
#matriz (completa)
#dados_train (treino)
#dados_test (teste)
# 80% treinamento e 20% teste
# random_state para garantir a reprodutibilidade
from sklearn.model_selection import train_test_split

# Separação
dados_train, dados_test = train_test_split(matriz,
                                           test_size=0.2,
                                           random_state=35)

# Conferência
print("Matriz completa: {} instâncias\n{} instâncias de treino\n{} instâncias de teste".
      format(len(matriz), len(dados_train), len(dados_test)))

"""# 3. Escalonamento - zscore"""

#Escalonamento - z-score
#Escalonar no treino e guardar a média e o desvio padrão para aplicar nos dados de teste.
# O StandardScaler aplica o Z-Score em cada variável, mas como é preciso guardar média e desvio, foi feito de maneira manual.
# Z-score: subtrai a média (centraliza os dados em 0) e divide pelo desvio padrão (ajusta a escala para variância 1)
# o StandardScaler, na prática, está fazendo o Z-Score em todas as colunas numéricas escolhidas
import pandas as pd
import numpy as np

# Seleciona apenas colunas numéricas
colunas_numericas = matriz.select_dtypes(include=np.number).columns

# Calcula média e desvio padrão apenas com os dados de treino
medias = dados_train[colunas_numericas].mean()
desvios = dados_train[colunas_numericas].std()

# Salva essas informações
media_desvio_dict = {
    'medias': medias,
    'desvios': desvios
}

# Faz o escalonamento do treino
dados_train_escalado = dados_train.copy()
dados_train_escalado[colunas_numericas] = (dados_train[colunas_numericas] - medias) / desvios

# Faz o escalonamento do teste usando médias e desvios do treino
dados_test_escalado = dados_test.copy()
dados_test_escalado[colunas_numericas] = (dados_test[colunas_numericas] - medias) / desvios

"""# 4. Multicolinearidade - Análise após o escalonamento"""

matriz=dados_train_escalado

"""## 4.1. VIF (Variance Inflation Factor) para multicolinearidade

O VIF (Variance Inflation Factor) é uma métrica utilizada para verificar a multicolinearidade em um conjunto de dados, ou seja, a correlação entre variáveis independentes. Quando variáveis independentes estão altamente correlacionadas entre si, a multicolinearidade pode distorcer as estimativas de um modelo de regressão, tornando-o menos confiável.

* O VIF mede o quanto a variabilidade de uma variável independente está inflacionada devido à correlação com outras variáveis independentes.

* Um VIF alto (geralmente maior que 5 ou 10, dependendo da referência) indica multicolinearidade alta, o que pode afetar a precisão dos coeficientes do model
"""

# VIF (Variance Inflation Factor) para multicolinearidade
# Valores de VIF > 5 (ou 10, a depender da referência) indicam multicolinearidade alta.
from statsmodels.stats.outliers_influence import variance_inflation_factor
from statsmodels.tools.tools import add_constant
import pandas as pd

# Seleciona variáveis numéricas
X = matriz.select_dtypes(include='number').dropna()  # Remove linhas com NaN

# Adiciona constante para o cálculo do VIF
X_const = add_constant(X)

# Calcula VIF
vif = pd.DataFrame()
vif['Variável'] = X.columns
vif['VIF'] = [variance_inflation_factor(X_const.values, i + 1) for i in range(len(X.columns))]

print("\nVIF (Variance Inflation Factor):")
print(vif.sort_values(by='VIF', ascending=False))

"""## 4.2. Matriz de correlação"""

# matriz de correlação
#Quando duas variáveis têm correlação muito alta (ex: > 0,9 ou < -0,9), pode ser problema.

import seaborn as sns
import matplotlib.pyplot as plt

# Calcular a matriz de correlação
correlacao = matriz.corr(numeric_only=True)

# Visualizar
plt.figure(figsize=(12, 8))
sns.heatmap(correlacao, annot=True, cmap='coolwarm')
plt.title('Matriz de Correlação')
plt.show()

# Como resolver a multicolinearidade:
  #Remover uma das variáveis altamente correlacionadas.
  #Combinar variáveis correlacionadas (ex: média, soma).
  #Reduzir Dimensionalidade: usar técnicas como PCA (Principal Component Analysis).
  #Modelos robustos: alguns algoritmos lidam bem com multicolinearidade (ex: árvore de decisão).

"""## 4.3. Determinante da matriz de correlação"""

# Determinante da matriz de correlação
# Se o determinante for próximo de 0 → existe multicolinearidade.
import numpy as np

corr_matrix = matriz.corr(numeric_only=True)
determinante = np.linalg.det(corr_matrix)

print(f"Determinante da matriz de correlação: {determinante:.6f}")

"""## 4.4. Eigenvalues (autovalores) da matriz de correlação"""

#Eigenvalues (autovalores) da matriz de correlação
#Quando um ou mais autovalores são muito pequenos (próximos de zero), indica que há combinações lineares entre variáveis.
eigenvalues, _ = np.linalg.eig(corr_matrix)

print("Autovalores:")
print(eigenvalues)

"""# 5. Solução para multicolinearidade - PCA"""

from sklearn.decomposition import PCA
import pandas as pd

# Aplica PCA para 95% da variância explicada
pca = PCA(n_components=0.95, random_state=42)
dados_train_pca = pca.fit_transform(dados_train_escalado)
dados_test_pca = pca.transform(dados_test_escalado)

# Converte para DataFrame (opcional, se quiser trabalhar com nomes de colunas)
dados_train_pca = pd.DataFrame(dados_train_pca, columns=[f'PC{i+1}' for i in range(dados_train_pca.shape[1])])
dados_test_pca = pd.DataFrame(dados_test_pca, columns=[f'PC{i+1}' for i in range(dados_test_pca.shape[1])])

# Exibe as informações
print(f"Número de colunas antes do PCA: {dados_train_escalado.shape[1]}")
print(f"Número de componentes principais após o PCA: {dados_train_pca.shape[1]}")

# Variância explicada por componente (em %)
variancia = pca.explained_variance_ratio_ * 100
print("\nVariância explicada por cada componente (em %):")
print(variancia.round(2))

"""# 6. Salvar as matrizes de treinamento e teste"""

from google.colab import files

# Treinamento
# Salvar o DataFrame de treino como CSV
dados_train_pca.to_csv('matriz_preprocessing_train.csv', index=False)

# Fazer o download do arquivo CSV
files.download('matriz_preprocessing_train.csv')

# Teste
# Salvar o DataFrame de teste como CSV
dados_test_pca.to_csv('matriz_preprocessing_test.csv', index=False)

# Fazer o download do arquivo CSV
files.download('matriz_preprocessing_test.csv')